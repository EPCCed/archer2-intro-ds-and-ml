{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c37f142e-0ba5-45ee-896b-3e2ef32b6f60",
   "metadata": {},
   "source": [
    "# __Neural Networks in Scikit-Learn and NumPy (Part B)__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cbb9e6-1c90-486f-bb50-30bd809862a9",
   "metadata": {},
   "source": [
    "In _Part A_, we looked at the Scikit-Learn Multi-layer Perceptron (MLP) object for building a neural network classifier.\n",
    "\n",
    "In this practical we drop down to a lower level, writing our own functions to build and train a neural network for the same problem. We use NumPy functions to implement things like matrix algebra, random number generation, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f1f8e6-3ab5-463b-8b4e-6da709373b9c",
   "metadata": {},
   "source": [
    "__As before from Part A__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3f2d4b-df9d-4ded-bdab-c7ff217394e7",
   "metadata": {},
   "source": [
    "General imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85b7ed5-5bf0-4da5-a9a4-2433000ed4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "import numpy as np\n",
    "from sklearn.datasets import make_moons\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b23bac-bcfc-43f7-93c1-a6a052ac2681",
   "metadata": {},
   "source": [
    "Generate some tricky data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c06560c-36ec-48a6-867b-afde9fb78a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples = 500, noise = 0.2, random_state = 101)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cdbc35-e432-452e-b38a-127867ff5815",
   "metadata": {},
   "source": [
    "Split the data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1de2bd-e666-47f8-a509-433c2a550187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert code here to split the data into 80:20 training and test sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41af197-1c5d-4653-a415-ad5f6ef6b585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise the training data\n",
    "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X_train[:,0], X_train[:,1], c = y_train, edgecolors='k', cmap = cm_bright)\n",
    "plt.title('Training data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2d15ff-4097-49d6-88c3-d8601020cad3",
   "metadata": {},
   "source": [
    "__New for Part B__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b900e7be-fdce-4c6b-baa2-e424dd2f8dbc",
   "metadata": {},
   "source": [
    "To build a neural network classifier at a lower level than available in Scikit-Learn, we need to think first about the \"ingredients\" that we require.\n",
    "- Definition of the model to do a __forward pass__: $y = \\sigma(W^{[L-1]}(\\sigma(W^{[L-2]}(... \\sigma(W^{[1]}x + b^{[1]})...)+b^{[L-2]}))+b^{[L-1]})$ where \n",
    "    - $\\sigma$ is the __activation function__\n",
    "    - $Wx + b$ is a linear function that goes from $\\mathbb{R}^n$ to $\\mathbb{R}^m$.\n",
    "    - In our example, the output of the model is a value between 0 and 1 that tells the probability of a point being blue or red. The input of the model are the coordinates of the point. \n",
    "- Definition of a __cost function__ that tells how good is the model in terms of its parameters: $$J(\\theta), \\text{where } \\theta:=\\{W^{[1]}, b^{[1]}, \\ldots, W^{[L-1]}, b^{[L-1]}\\}$$\n",
    "- __Optimisation__ of the cost function using gradient descent.\n",
    "    - We need to apply the chain rule (backpropagation) in order to obtain $\\partial_{\\theta}J$ for each optimisation step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f938210-2d79-4dff-855d-c3529136e28b",
   "metadata": {},
   "source": [
    "__Activation Function__ - sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc3ac0a-6145-4040-b903-51795589dbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    output = 1/(1+np.exp(-z))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9328a234-06f2-4631-b988-87313749efc6",
   "metadata": {},
   "source": [
    "__Forward Model Definition__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f598ef3-9c80-4153-9422-3ac9aaad4ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_model(model, x):\n",
    "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "    \n",
    "    # forward pass\n",
    "    # we use a single hidden layer\n",
    "    # complete the \"a2 =\" line using the activation function\n",
    "    z2 = np.matmul(x,W1) + b1\n",
    "    a2 = \n",
    "    \n",
    "    # output layer\n",
    "    # complete the \"a3 =\" line using the activation function\n",
    "    z3 = np.matmul(a2,W2) + b2\n",
    "    a3 = \n",
    "    \n",
    "    #return the output of the model (a3) and the intermediate layers\n",
    "    return z2, a2, z3, a3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458007cc-20e1-4c68-8cc7-892351783660",
   "metadata": {},
   "source": [
    "__Loss function__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef982c6a-5f1c-40f6-a688-d4244fa7e5b3",
   "metadata": {},
   "source": [
    "Let $\\mathcal{D} = \\{(x^1, y^1), (x^2, y^2), \\ldots, (x^N, y^N)\\}$ be our training set, where $x^i\\in \\mathbb{R}^n$\n",
    "We define the loss function as $$J(\\theta) = \\sum_{i=1}^{N} y^i \\log(f_{\\theta}(x^i)) + (1-y^i) \\log(1-f_{\\theta}(x^i))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eab7b33-9876-4e27-9851-b6dbb7e773f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(model, x, y):\n",
    "    _,_,_,y_pred = forward_model(model, x)\n",
    "    loss_batch = y * np.log(y_pred) + (1-y) * np.log(1-y_pred)\n",
    "    # complete the line below to calculate the loss\n",
    "    loss = \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f739e2-c8c8-417c-bc67-575da70dc36f",
   "metadata": {},
   "source": [
    "__Optimisation__ Gradient Descent algorithm\n",
    "- Update the weights and biases in the neural network according to the gradient of the cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832b0939-bc02-428d-8e0b-bd9b32fc8d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GD_step(model, x, y, lr = 0.001):\n",
    "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "    \n",
    "    z2, a2, z3, a3 = forward_model(model, x)\n",
    "    \n",
    "    delta3 = a3-y\n",
    "    dW2 = np.matmul(a2.T,delta3)\n",
    "    db2 = np.sum(delta3, axis=0, keepdims=True)\n",
    "    \n",
    "    delta2 = sigmoid(z2)*(1-sigmoid(z2)) * delta3.dot(W2.T)\n",
    "    dW1 = np.matmul(x.T, delta2)\n",
    "    db1 = np.sum(delta2, axis=0)\n",
    "    \n",
    "    W2 = W2 - lr * dW2\n",
    "    b2 = b2 - lr * db2\n",
    "    W1 = W1 - lr * dW1\n",
    "    b1 = b1 - lr * db1\n",
    "    \n",
    "    model['W1'], model['b1'], model['W2'], model['b2'] = W1, b1, W2, b2\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c398328a-d0f6-4c47-9cc2-4fe5984303f5",
   "metadata": {},
   "source": [
    "#### We put everything together for the training:\n",
    "- Initialise $W^{[1]}, b^{[1]}, \\ldots, W^{[L-1]}, b^{[L-1]}$.\n",
    "- While Not convergence:\n",
    "    - Calculate $J(\\theta)$\n",
    "    - Update $W^{[i]} := W^{[i]} - \\alpha \\cdot \\partial_{W^{[i]}}J$\n",
    "    - Update $b^{[i]} := b^{[i]} - \\alpha \\cdot \\partial_{b^{[i]}}J$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ec4d77-a2c5-4c83-80db-be874980dc03",
   "metadata": {},
   "source": [
    "__Define the training function__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc5c615-270a-471b-bdc3-e24927bee2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, n_epochs, X, y):\n",
    "    # for a pre-defined number of epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        # update weights and biases\n",
    "        model = GD_step(model, x=X, y=y)\n",
    "        # calculate loss\n",
    "        loss = loss_fn(model, x=X, y=y)\n",
    "        # print information every 100 epochs\n",
    "        if epoch%100 == 0:\n",
    "            print(\"Epoch: {}/{}, loss: {}\".format(epoch, n_epochs, loss))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8691b2-1e88-4252-bee8-990999a0107c",
   "metadata": {},
   "source": [
    "__Now run the training__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d5e098-3c8d-4826-9d41-94fb685283b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n_hidden = 30\n",
    "# Initialise weights and biases\n",
    "W1 = np.random.randn(2, n_hidden)\n",
    "b1 = np.random.randn(1, n_hidden)\n",
    "W2 = np.random.randn(n_hidden, 1)\n",
    "b2 = np.random.randn(1,1)\n",
    "\n",
    "#The above functions expect to receive 'model' as a Python Dictionary object.\n",
    "#Define it here\n",
    "model = \n",
    "#Populate the model with the initialised weights and biases from above\n",
    "#<enter code here>\n",
    "\n",
    "# Train for 10000 epochs\n",
    "model = train(model=model, n_epochs=10000, X=X, y=y.reshape(500,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e17dac9-5132-425d-9b77-3722a5c1653b",
   "metadata": {},
   "source": [
    "__Accuracy Metric:__\n",
    "\n",
    "- _Use ideas from the code in Practical8a.ipynb to calculate the accuracy of the model._\n",
    "\n",
    "- _Can you write this this code as a function and incorporate it into the print statement in the training function to track the accuracy during optimisation._\n",
    "    - _remember, the loss function, not the accuracy metric, is used to improve the model!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030f7d75-26f3-4875-91c5-3943d998048e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#<enter code here>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c0b9ba-4a89-4ebb-8285-65b55ea99f07",
   "metadata": {},
   "source": [
    "__Visualise:__\n",
    "- _Adapt code from the from Practical8a.ipynb to plot the decision boundaries of the model_\n",
    "- _Do the boundaries make more sense visually?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5108b074-ed29-46da-aa54-924771357285",
   "metadata": {},
   "outputs": [],
   "source": [
    "#<enter code here>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
