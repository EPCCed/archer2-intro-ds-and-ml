{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c37f142e-0ba5-45ee-896b-3e2ef32b6f60",
   "metadata": {},
   "source": [
    "# Neural Networks with a Deep Learning Framework - PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cbb9e6-1c90-486f-bb50-30bd809862a9",
   "metadata": {},
   "source": [
    "In Practical9a.ipynb, we will first look at the same problem from the Practical 8 - classification of sklearn \"moons\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3f2d4b-df9d-4ded-bdab-c7ff217394e7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "General imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85b7ed5-5bf0-4da5-a9a4-2433000ed4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "import numpy as np\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b23bac-bcfc-43f7-93c1-a6a052ac2681",
   "metadata": {},
   "source": [
    "Generate \"moons\" dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c06560c-36ec-48a6-867b-afde9fb78a6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples = 500, noise = 0.2, random_state = 101)\n",
    "\n",
    "# visualisation\n",
    "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[:,0], X[:,1], c = y, edgecolors='k', cmap = cm_bright)\n",
    "plt.show(); # NB: adding \";\" to the end\n",
    "print(X); # NB: adding \";\" to the end\n",
    "print(y); # NB: adding \";\" to the end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8726a1-60be-4636-ad75-dadd5582bb5a",
   "metadata": {},
   "source": [
    "## Before jumping into making a neural network, let's look at PyTorch Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab27d325-2b2f-40ab-87c5-54a58a95338d",
   "metadata": {},
   "source": [
    "- __Tensors__\n",
    "    - Tensors are similar to NumPy’s ndarrays, with the addition being that Tensors can also be used on a GPU to accelerate computing. \n",
    "    - To aid usability for those used to using NumPY, PyTorch adopt a similar API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a03e9bb-9d6d-4533-af70-c75709e679df",
   "metadata": {},
   "source": [
    "Let's look at creating NumPy arrays and PyTorch Tensors and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49ce12b-81b1-41f8-bfd5-95f0ecc8eaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy array construction from a Python list\n",
    "my_list = [[1,2,3],[4,5,6]]\n",
    "first_array = np.array(my_list) # 2x3 array\n",
    "print(\"Array Type: {}\".format(type(first_array))) # type\n",
    "print(\"Array Shape: {}\".format(np.shape(first_array))) # shape\n",
    "print(first_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b129c0f-9875-4aa3-b0dc-9f15e3fdcb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch Tensor construction from a Python list\n",
    "tensor = torch.Tensor(my_list)\n",
    "print(\"Array Type: {}\".format(tensor.type)) # type\n",
    "print(\"Array Shape: {}\".format(tensor.shape)) # shape\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0b87a1-31e8-41e3-9dcb-bc123b27a5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty arrays and Tensors are equivalent\n",
    "print('numpy ones')\n",
    "print(np.empty((2,3)))\n",
    "\n",
    "print('pytorch ones')\n",
    "print(torch.empty((2,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5095660b-7831-4cee-b549-6f576d753491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arrays and Tensors of ones are equivalent\n",
    "print('numpy ones')\n",
    "print(np.ones((2,3)))\n",
    "\n",
    "print('pytorch ones')\n",
    "print(torch.ones((2,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994391e9-bb57-4a4f-b90d-a5e683f3b35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arrays and Tensors of random numbers are equivalent\n",
    "print('numpy random')\n",
    "print(np.random.rand(2,3)) # not fewer parentheses here - `np.random.rand()` takes two integers as inputs, whereas `np.ones()` takes a shape tuple\n",
    "\n",
    "print('pytorch ones')\n",
    "print(torch.rand((2,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa66165-7a13-4f2d-9704-3072d6b0620f",
   "metadata": {},
   "source": [
    "We can also convert between NumPy arrays and PyTorch Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31336705-fb6d-4343-b520-77667e6a5bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from numpy to tensor\n",
    "array = np.array([[1,2,3],[4,5,6]])\n",
    "print(type(array))\n",
    "print(array)\n",
    "tensor = torch.from_numpy(array)\n",
    "print(type(tensor))\n",
    "print(tensor)\n",
    "\n",
    "# from tensor to numpy\n",
    "array2 = tensor.numpy()\n",
    "print(type(array2))\n",
    "print(array2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbbceb5-0281-4d6c-bd5a-0292bf61f90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we often need to see the shape of an array or tensor\n",
    "print(array.shape)\n",
    "print(tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced45c6b-b21f-48cc-aa46-f51b679d95ef",
   "metadata": {},
   "source": [
    "## Building a neural network - `tensor`, `autograd`, `torch.nn`, `forward()` and `backward()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dd191e-0721-4b1a-b3b1-5502693a2c8e",
   "metadata": {},
   "source": [
    "### Neural Networks\n",
    "\n",
    "\n",
    "Neural networks can be constructed using the `torch.nn` package.\n",
    "\n",
    "- `nn` depends on `autograd` to define models and differentiate them ())producing gradients).\n",
    "- An `nn.Module` contains layers, and a method `forward(input)` that\n",
    "returns the `output`.\n",
    "\n",
    "\n",
    "A typical training procedure for a neural network is as follows:\n",
    "\n",
    "- Define the neural network that has some learnable parameters (or\n",
    "  weights)\n",
    "- Iterate over a dataset of inputs\n",
    "- Process input through the network\n",
    "- Compute the loss (how far is the output from being correct)\n",
    "- Propagate gradients back into the network’s parameters\n",
    "- Update the weights of the network, typically using a simple update rule:\n",
    "  ``weight = weight - learning_rate * gradient``\n",
    "\n",
    "### Define the network\n",
    "\n",
    "\n",
    "Let’s define this network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424b0eb2-562b-49d5-9424-2869fd5f6d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN(nn.Module): ## construct a class extending nn.Module\n",
    "    \n",
    "    def __init__(self, n_hidden):\n",
    "        super(ANN, self).__init__()\n",
    "        self.i_h = nn.Sequential(nn.Linear(2, n_hidden), nn.Sigmoid()) # input layer taking 2 features (x and y coordinates of a point) and producing n_hidden features\n",
    "        self.h_o = nn.Sequential(nn.Linear(n_hidden, 1), nn.Sigmoid()) # output layer taking n_hidden features and producing 1 feature (the classification)\n",
    "        \n",
    "    def forward(self, x): # produce output given input by using input and output layers\n",
    "        h = self.i_h(x)\n",
    "        output = self.h_o(h)\n",
    "        return output\n",
    "    \n",
    "n_hidden = 30\n",
    "net = ANN(n_hidden = n_hidden) # initialse an ANN object with n_hidden = 30 \n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63bfe2a-95a3-4fb2-89fc-d9a10961c4ca",
   "metadata": {},
   "source": [
    "We have defined the `forward()` function. `autograd` will define the `backward()` function for us in the following steps.\n",
    "\n",
    "To see the tensors for features, weights and biases of our layers, use `state_dict()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c8b583-454b-4cca-8077-c1166dfd6ecb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(net.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3238950-648f-4767-8f94-34c4f7267051",
   "metadata": {},
   "source": [
    "With our nerual network setup, we now need a loss function.\n",
    "\n",
    "`torch.nn` provides a number of predefined loss functions [https://pytorch.org/docs/stable/nn.html#loss-functions](), of which we will choose the Binary Cross-entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55bfa41-93e9-4cb4-8b86-52c96f404e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data to tensor\n",
    "X_tensor = torch.tensor(X, dtype = torch.float)\n",
    "target = torch.tensor(y, dtype=torch.float32).view(-1,1) # `torch.tensor.view()` is equivalent to np.reshape() - here we swap rows and columns to get y into the shape we need\n",
    "\n",
    "y_output = net(X_tensor) # calculating y_output produces the predicted y values\n",
    "loss_fn = nn.BCELoss(reduction=\"sum\")\n",
    "loss = loss_fn(y_output, target) # the loss is the difference between predicted and target values according to the loss function\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06841f0c-44ac-4d40-88d1-a400b11ded9e",
   "metadata": {},
   "source": [
    "### Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0e78ef-42f3-4504-9364-92311feaba60",
   "metadata": {},
   "source": [
    "To backpropagate:\n",
    "- clear previous gradients in the neural network\n",
    "- use `loss.backward()` to backpropagate the errors.\n",
    "\n",
    "That's it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43d7734-857d-48f1-8980-99c65747313c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#note, running this cell more than once will raise a runtime error as loss.backward() should only be run once.\n",
    "net.zero_grad()\n",
    "print([p.grad for p in net.parameters()])\n",
    "loss.backward()\n",
    "print([p.grad for p in net.parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b89a115-c6ab-4b10-8cce-4082e43f5c42",
   "metadata": {},
   "source": [
    "### Update weights - optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7690e7a-d536-4c95-b02d-628329521ae4",
   "metadata": {},
   "source": [
    "- in the previous practical we defined a Stochastic Gradient Descent optimiser in NumPy\n",
    "- with PyTorch, we will simply chose the optimiser from `torch.optim`\n",
    "- in `torch.optim.SGD()` we provide the parameters of the model, the learning rate (lr) and the momentum\n",
    "- for more reading on initialisation of lr and momentum, see [http://www.cs.toronto.edu/%7Ehinton/absps/momentum.pdf](http://www.cs.toronto.edu/%7Ehinton/absps/momentum.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3f17c8-8891-4a13-8a01-d3cabc734928",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfb4596-78c4-431a-8abb-f685e2da2ced",
   "metadata": {},
   "source": [
    "- _Try printing the parameters of the model before an after making an optimisation step (`optimiser.step()`) to see the difference._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24525138-0e92-4414-b394-8f282c0ccd54",
   "metadata": {},
   "source": [
    "## Run the model optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a573f5dc-db9e-4a01-aa32-45367b682175",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n_hidden = 30\n",
    "net = ANN(n_hidden = n_hidden)\n",
    "\n",
    "# optimizer\n",
    "lr = 0.001\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.)\n",
    "\n",
    "# loss function\n",
    "loss_fn = torch.nn.BCELoss(reduction=\"sum\")\n",
    "\n",
    "# data to tensor\n",
    "X_tensor = torch.tensor(X, dtype = torch.float)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32).view(-1,1)\n",
    "\n",
    "# training\n",
    "n_epochs = 10000\n",
    "for epoch in range(n_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = net(X_tensor)\n",
    "    loss = loss_fn(y_pred, y_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch%100 == 0:\n",
    "        print(\"Iteration {}/{}, loss {:.4f}\".format(epoch, n_epochs, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d466a93c-f518-4bfa-b968-eb7a32d4cd48",
   "metadata": {},
   "source": [
    "__Plot the resulting decision boundaries of the model__\n",
    "\n",
    "__Can you understand each line of the cell below? - What is different now we are using PyTorch?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87b654b-4748-43e7-b366-96cacf10e61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting \n",
    "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "h = 0.02 # step in the mesh\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "xx_tensor, yy_tensor = torch.tensor(xx, dtype=torch.float), torch.tensor(yy, dtype=torch.float)\n",
    "Z = net(torch.stack([xx_tensor.flatten(), yy_tensor.flatten()], 1))\n",
    "Z = Z.detach().numpy()\n",
    "Z = Z.reshape(xx.shape)\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[:,0], X[:,1], c = y.ravel(), edgecolors='k', cmap = cm_bright)\n",
    "Z_binary = Z>=0.5\n",
    "Z_binary = Z_binary.reshape(xx.shape)\n",
    "ax.contourf(xx, yy, Z_binary, cmap=cm_bright, alpha=.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7ca68f-f663-4b06-a032-6755a561eb18",
   "metadata": {},
   "source": [
    "## This was equivalent to the NumPy training in the previous practical in terms of the number of epochs, features, optimiser and loss function\n",
    "\n",
    "## Did it perform more quickly?\n",
    "Remember, PyTorch tensors are equivalent to NumPy arrays, but they can be used efficiently on GPUs - _should_ this optimisation run more quickly than the NumPy-based one?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f29fd47-c5e0-4336-b30a-878428169f45",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d030f89-2f26-4751-9036-8d7d6327f886",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
