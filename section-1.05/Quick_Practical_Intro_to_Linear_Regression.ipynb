{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A quick introduction to Linear Regression in Python\n",
    "#### Pratical Introduction to Data Science\n",
    "\n",
    "This brief practical is intended to provide you with a starting point to experiment with some aspects of linear regression. It's also worth trying to see whether you can convert some of the content from lectures that is expressed into mathematical notation into code.\n",
    "\n",
    "As you evaluate the cells below, try to make sure you understand what's happening at each stage. I encourage you to tweak the code to experiment with what happens if you do things slightly differently (e.g. if you swap Series with slices from DataFrames, etc.).\n",
    "\n",
    "## Linear Regression in 2 Dimensions\n",
    "\n",
    "We'll start by looking at the familiar 2D case (where $y$ is a function of a single variable $x$).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #pandas to give us dataframes\n",
    "import seaborn as sns #seaborn for some prettier plots\n",
    "import numpy as np #used later for matrix manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file `cats.csv` should already be in your folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data from `cats.csv` into a DataFrame called `cats`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats=pd.read_csv(\"cats.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third and fourth columns in the cats table refer to body weight (Bwt) and heart weight (Hwt).\n",
    "Note, the body weights are in kilograms, whereas the heart weights are in grammes.\n",
    "We're going to build a model that predicts a cat's heart weight from its body weight.\n",
    "\n",
    "First we'll look at how you can do linear regression using *scikit-learn*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to plot the body weight (x-axis)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats.iloc[:,2:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...against the heart weight (y-weight):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats.iloc[:,3:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note that we're going to pass in slices of the DataFrame, as shown above, rather than series.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll perform a Linear Regression to find the line of best fit. We'll store our result in `reg`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg=LinearRegression().fit(cats.iloc[:,2:3],cats.iloc[:,3:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the line below if you want to see the error that you get if you pass in series\n",
    "#reg=LinearRegression().fit(cats.iloc[:,2],cats.iloc[:,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt # We'll use matplotlib to create some plots of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the original data (without any regression line shown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(cats.iloc[:,2],cats.iloc[:,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can obtain the coefficient for the linear regression (the \"slope\") from the `reg` object that we obtained from doing the `fit`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And similarly, we can get the intercept on the $y$-axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a brief aside, we can also see the arguments which `fit` was originally called with, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's manually get a predicted value of $y$ for each $x$ point using the normal formula for a straight line, $y=mx+c$ where $y$ corresponds to the *predicted* value for `Hwt`, $x$ is the `Bwt`, $m$ is `reg.coef_` and $c$ is `reg.intercept_`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred=cats.iloc[:,2:3]*reg.coef_+reg.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll know plot the original data, and superimpose the straight line that comes from `ypred`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(cats.iloc[:,2:3],cats.iloc[:,3:4])\n",
    "plt.plot(cats['Bwt'],ypred['Bwt'], color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of evaluating the formula itself, you can use the built in `predict` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred2=reg.predict(cats.iloc[:,2:3])\n",
    "ypred2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...which gives the same results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(cats.iloc[:,2:3],cats.iloc[:,3:4])\n",
    "plt.plot(cats['Bwt'],ypred2, color='green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we've allowed LinearRegression.fit() to find our straight line that best fits the data. To convince yourself that what you're plotting is a straight line with the calculated slope and intercept, try changing the values of a and b below, recalcuating ymanual and replotting the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=-0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ymanual= a*cats.iloc[:,2:3]+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(cats.iloc[:,2:3],cats.iloc[:,3:4])\n",
    "plt.plot(cats['Bwt'],ypred['Bwt'], color='red')\n",
    "plt.plot(cats['Bwt'],ymanual['Bwt'], color='green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later on in this practical, we're going to see how you can find the values of `a` and `b` that give the best fit from first principles, instead of using `fit()` from `sklearn`.\n",
    "\n",
    "### Calculating the Cost Function\n",
    "\n",
    "You will remember that Linear Regression works by minimising a **cost function** based on the **Mean Squared Error**. \n",
    "\n",
    "The cost function is calculated repeatedly in the process of fitting (or, to use Machine Learning terminology, in learning the model).\n",
    "\n",
    "Let's calculate the Mean Square Error here.\n",
    "\n",
    "We're looking at a simple case here where the target variable, $y$, depends on only one variable, $x$.\n",
    "\n",
    "Our model can be expressed as $ h_\\theta(x) = \\theta_0+\\theta_1 x $.\n",
    "\n",
    "The points in the dataset above can be expressed as the set $\\{(x^{(i)},y^{(i)})\\}$ where $i$ runs from $1$ to $m$.\n",
    "\n",
    "In the code above the slope, $\\theta_1$, is represented by the variable `a`, and the intercept, $\\theta_0$, is represented by the variable `b`.\n",
    "\n",
    "The usual expression for the cost function is as follows:\n",
    "\n",
    "$$ J(\\theta_0, \\theta_1) = \\frac{1}{2m}\\sum_{i=1}^m (h_\\theta(x^{(i)})-y^{(i)})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK**: Write some code which calculates this quantity for the dataset above. Remember $h_\\theta(x)$ is the function for the straight line. I'm not asking here for code to *minimise* this function, just to *calculate* it given your straight line (defined by your parameters $\\mathbf{\\theta}$) and the original data points.\n",
    "\n",
    "**Hint:** Depending on your approach, you may need to convert a pandas dataframe to a numpy array. If you need to do this you can use `to_numpy()`.\n",
    "\n",
    "You should get the answer `1.040045641450006`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an aside, you can also calculate the mean square error using sklearn. Note that I said that cost function above was *based* on the mean squared error. The actual mean squared error does not have the factor of $2$ in the denominator. (It's often included in the cost function because in code, when you're minimising a quantity, you calculate its derivative, and the factor of two cancels out in the calculation making it more efficient)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(cats.iloc[:,3:4],ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Some quick exploratory data analysis\n",
    "\n",
    "We are now going to look at the public dataset *mtcars* (this is another example dataset that is included with R).  This data was extracted from the 1974 Motor Trend US magazine, and comprises fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (1973-74 models).\n",
    "\n",
    "This dataset is not built into Python, but we can download it directly via https://gist.githubusercontent.com/ZeccaLehn/4e06d2575eb9589dbe8c365d61cb056c/raw/64f1660f38ef523b2a1a13be77b002b98665cdfe/mtcars.csv .\n",
    "\n",
    "We've already done this for you however, so you can go ahead and simply read in the `mtcars.csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CSV mtcars\n",
    "mtcars = pd.read_csv('mtcars.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtcars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we can view basic descriptive statistics of this dataset using the `describe` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtcars.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's often informative to visualise data like this with a scatterplot matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(mtcars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seaborn provides a version that is a bit prettier out-of-the-box:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(mtcars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a great way to roughly determine if there is any linear correlation between any pair of variables. You can subset the plot to those variables that seem to have a linear correlation as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(mtcars[['mpg','wt','disp']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple variable linear regression\n",
    "\n",
    "Now we will explore linear regression using more than one variable.\n",
    "\n",
    "We'll move up from a function of *one* variable to a function of *two* variables. The technique works for any number of variables, but it gets very difficult to visualise in higher dimensions.\n",
    "\n",
    "Whereas simple linear regression gives a model corresponding a straight **line**, linear regression with two independent variables gives a model corresponding to a flat **plane** (and in general, in higher dimensions, a so-called hyperplane). The \"height\" of the plane corresponds to the value of the hypothesis function which defines the model.\n",
    "\n",
    "We're going to continue to work with the subset of the mtcars dataset that we looked at above, and in particular we will try to predict the number of miles a car can drive for each gallon of petrol (`mpg`) using the weight (`wt` in lb/1000) of the car and its engine displacement (`disp` in cu.in.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter( mtcars['wt'], mtcars['disp'],mtcars['mpg'], c='blue', s=60)\n",
    "\n",
    "# create a grid of values in the wt (p1) and disp (p2) directions\n",
    "# to help us draw a plane\n",
    "p1, p2 = np.mgrid[1:6, 0:500:100] \n",
    "\n",
    "a=-3.3\n",
    "b=-0.015\n",
    "c=34\n",
    "\n",
    "plane= a*p1 + b*p2 + c\n",
    "\n",
    "ax.plot_surface(p1,p2,plane,color='red',alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As promised above, we're now going to find the optimal parameters for this model (those that give the \"best fit\") without using library functions. This is so that you can see how linear regression works under-the-hood. The concept of **minimising a cost function** is very important in machine learning, and you will also see it in many classification algorithms including those that use neural networks.\n",
    "\n",
    "We will now try to use the matrix version of the normal equation (as described in lectures) to find the optimal parameters for this model. The matrix version requires two matrices X and y defined as:\n",
    "\n",
    "$$ X =\n",
    " \\begin{pmatrix}\n",
    "  1 & x_1^{(1)} & x_2^{(1)} & \\cdots & x_n^{(1)} \\\\\n",
    "  1 & x_1^{(2)} & x_2^{(2)} & \\cdots & x_n^{(2)} \\\\\n",
    "  \\vdots  & \\vdots & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "  1 & x_1^{(m)} & x_2^{(m)} & \\cdots & x_n^{(m)} \\\\\n",
    "\\end{pmatrix}, y = \n",
    "\\begin{pmatrix}\n",
    "  y^{(1)} \\\\\n",
    "  y^{(2)} \\\\\n",
    "  \\vdots \\\\\n",
    "  y^{(m)}\n",
    "\\end{pmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wish the $y$ matrix to the `mpg` values that our model will predict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=mtcars['mpg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TASK: See if you can start to build the X matrix with columns `wt` and `disp`. Your aim is to have an X matrix that looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![fig1.png](fig1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to solve for the parameters using the equation:\n",
    "\n",
    "$$\n",
    "\\mathbf{\\theta}=(X^TX)^{-1}X^Ty\n",
    "$$\n",
    "\n",
    "**TASK**: Build up this equation in Python. We _could_ convert things back to NumPy at the start (and for high performance with larger data, this might be desirable) but you can do some things like the transpose with Pandas direclty. The inverse, on the other hand, is easier to do in NumPy with `np.linalg.inv()`.\n",
    "\n",
    "Here are some hints:\n",
    "\n",
    "1. Both dataframes and numpy arrays have a transpose method e.g. `M.transpose()`\n",
    "2. Matrix multiplication can be performed with both dataframes and numpy arrays using the dot method, e.g. `A.dot(B)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the co-efficients returned correspond to a good fit when you plug them into the surface plot above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK**: Now see if you can use these coefficients to predict the mpg for the Volvo 142E using the weight and displacement figures shown above for that vehicle.\n",
    "\n",
    "Hint:\n",
    "\n",
    "- You can use matrix multiplication to multiply a vector containing the required values by the co-efficients matrix. Remember to include the value 1 as the first value of your 3-value vector.\n",
    "\n",
    "The predicted value for the Volvo 142E should be 23.50057 miles per gallon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple variable linear regression using built in functions\n",
    "\n",
    "We have been doing things \"by hand\" above to make sure you understand a little bit about how linear regression is implemented.\n",
    "\n",
    "In practice, it's usually much more appropriate to use existing methods, and the same functions used above for single variable linear regression also work for multiple variable regression.\n",
    "\n",
    "All the work that we have done above (and more) can be done as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg=LinearRegression().fit(mtcars[['wt','disp']],mtcars['mpg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.coef_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CHALLENGE** Look at the [documentation for `sklearn.metrics`](https://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics). See if you can find a way to calculate the statistics provided in the following output that the *R* language provides for linear regression models.\n",
    "\n",
    "```\n",
    "> summary(model)\n",
    "\n",
    "Call:\n",
    "lm(formula = mtcars$mpg ~ mtcars$wt + mtcars$disp)\n",
    "\n",
    "Residuals:\n",
    "    Min      1Q  Median      3Q     Max\n",
    "-3.4087 -2.3243 -0.7683  1.7721  6.3484\n",
    "\n",
    "Coefficients:\n",
    "            Estimate Std. Error t value Pr(>|t|)\n",
    "(Intercept) 34.96055    2.16454  16.151 4.91e-16 ***\n",
    "mtcars$wt   -3.35082    1.16413  -2.878  0.00743 **\n",
    "mtcars$disp -0.01773    0.00919  -1.929  0.06362 .\n",
    "---\n",
    "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
    "\n",
    "Residual standard error: 2.917 on 29 degrees of freedom\n",
    "Multiple R-squared:  0.7809,    Adjusted R-squared:  0.7658\n",
    "F-statistic: 51.69 on 2 and 29 DF,  p-value: 2.744e-10\n",
    "\n",
    "```\n",
    "\n",
    "This is easier for some of the metrics than others. Once you have spent a bit of time trying to do this, you can look at [this answer on StackOverflow](https://stackoverflow.com/a/57239611) which shows how to calculate some of the most useful statistics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
